{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOwIGscOg7Yx4yr/7+qkKOP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Brain f-MRI Image Classification of Alzheimer's Diagnosis Using Deep Learning Modeling CNNs"
      ],
      "metadata": {
        "id": "43d1AX7QBtll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "WGpj53wqcvZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AcyS1xblewrU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(72)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(72)"
      ],
      "metadata": {
        "id": "0A1t7MVCA4d6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l77I79jzBXAR",
        "outputId": "d4575727-e27a-4a0b-b9b0-e8faa77f0171"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Dataset"
      ],
      "metadata": {
        "id": "hkWypu2cc6_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory='/content/drive/MyDrive/data',\n",
        "    batch_size=6400,\n",
        "    seed=42,\n",
        "    image_size=(256, 256)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8JWpd_gRdAS",
        "outputId": "63a17396-4303-4df0-ab5c-40fcbe64fdb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6400 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(data))"
      ],
      "metadata": {
        "id": "rC3hfr12CX4C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "T06ew0SJDCJ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYBLQ_BzDOQs",
        "outputId": "9f12f056-6fba-422b-92fc-c4a1bbd6a948"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6400, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNrmH6imEzn8",
        "outputId": "38e66c97-1fd4-4286-8f3c-4a82fd596228"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6400,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, test_images, train_labels, test_labels = train_test_split(images,\n",
        "                                                                        labels,\n",
        "                                                                        test_size=0.3,\n",
        "                                                                        random_state=42)"
      ],
      "metadata": {
        "id": "MITvt9UZDSNd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images, val_images, test_labels, val_labels = train_test_split(test_images,\n",
        "                                                                    test_labels,\n",
        "                                                                    random_state=42,\n",
        "                                                                    test_size=0.5)"
      ],
      "metadata": {
        "id": "4uzDDr0sEP27"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = (256,256,3)"
      ],
      "metadata": {
        "id": "uEys7F_cN1Vf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing images"
      ],
      "metadata": {
        "id": "OxSfCshVazKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, test_images, val_images = train_images/255, test_images/255, val_images/255"
      ],
      "metadata": {
        "id": "7xFDF4BoapiZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions"
      ],
      "metadata": {
        "id": "RjusVnFr5-iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for evaluating "
      ],
      "metadata": {
        "id": "h4r_-BHF1ocw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, name, history, X, y, threshold=0.5):\n",
        "    \n",
        "    print(f\"Results for {name} with threshold = {threshold}.\")\n",
        "    \n",
        "    plt.rcParams.update({'font.size': 18})\n",
        "    #Create a function that provides useful vis for model\n",
        "    #performance. This is especially useful as we are most\n",
        "    #concerned with the number of false negatives\n",
        "    \n",
        "    train_loss=[value for key, value in history.items() if 'loss' in key.lower()][0]\n",
        "    valid_loss=[value for key, value in history.items() if 'loss' in key.lower()][1]\n",
        "    train_auc=[value for key, value in history.items() if 'auc' in key.lower()][0]\n",
        "    valid_auc=[value for key, value in history.items() if 'auc' in key.lower()][1]\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(25, 15))\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.plot(train_loss, color='tab:blue', label='Train Loss')\n",
        "    ax1.plot(valid_loss, color='tab:orange', label='Valid Loss')\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.set_ylim([0,1.05])\n",
        "    plt.title('Model Loss')\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('AUC')\n",
        "    ax2.plot(train_auc, color='tab:blue', label='Train AUC')\n",
        "    ax2.plot(valid_auc, color='tab:orange', label='Valid AUC')\n",
        "    ax2.legend(loc='upper left')\n",
        "    ax2.set_ylim([0.5,1.05])\n",
        "    plt.title('Model AUC')\n",
        "        \n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_adjusted = np.zeros([len(y), ])\n",
        "    i=0\n",
        "    for pred in y_pred:\n",
        "        if pred > threshold:\n",
        "            y_pred_adjusted[i] = 1\n",
        "            i+=1\n",
        "        else:\n",
        "            y_pred_adjusted[i] = 0\n",
        "            i+=1\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred_adjusted)\n",
        "    cm_df = pd.DataFrame(cm)\n",
        "\n",
        "    sns.heatmap(cm, ax=ax3, annot=True, cmap='Blues', fmt='0.7g') \n",
        "\n",
        "    plt.sca(ax3)\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('True Values')\n",
        "    \n",
        "    train_recall=[value for key, value in history.items() if 'recall' in key.lower()][0]\n",
        "    valid_recall=[value for key, value in history.items() if 'recall' in key.lower()][1]\n",
        "    train_precision=[value for key, value in history.items() if 'precision' in key.lower()][0]\n",
        "    valid_precision=[value for key, value in history.items() if 'precision' in key.lower()][1]\n",
        "    \n",
        "    if (cm_df[0][1] == 0) or (cm_df[1][1] ==0):\n",
        "        train_f1 = 'N/A'\n",
        "        valid_f1 = 'N/A'\n",
        "    else:\n",
        "        train_f1 = 2*(train_recall[-1]*train_precision[-1])/(train_recall[-1]+train_precision[-1])\n",
        "        valid_f1 = 2*(valid_recall[-1]*valid_precision[-1])/(valid_recall[-1]+valid_precision[-1])\n",
        "    \n",
        "    print(f\"\\n Train f1: {train_f1} \\n Val f1: {valid_f1} \\n\\n Train Recall: {train_recall[-1]} \\n Val Recall: {valid_recall[-1]}\")\n",
        "\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Recall')\n",
        "    ax4.set_ylim([-0.05,1.05])\n",
        "    ax4.plot(train_recall, '--', color='tab:orange', label='Train Recall')\n",
        "    ax4.plot(valid_recall, color='tab:orange', label='Valid Recall')\n",
        "    ax4.legend(loc='lower left')\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OW6H-GvOCmPc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "vx-I7g74Cjx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Simple Model"
      ],
      "metadata": {
        "id": "zOu60c4MpL_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = models.Sequential([\n",
        "    # since Conv2D is the first layer of the neural network, we should also specify the size of the input\n",
        "    layers.Conv2D(128, (3,3), activation='relu', input_shape=(input_layer)),\n",
        "    # flatten the result to feed it to the dense layer\n",
        "    layers.Flatten(), \n",
        "    # and define 512 neurons for processing the output coming by the previous layers\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    # a single output neuron\n",
        "    layers.Dense(1, activation='sigmoid')  \n",
        "])"
      ],
      "metadata": {
        "id": "vF1ozajzplco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['accuracy', 'AUC', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]"
      ],
      "metadata": {
        "id": "l2MRyQ9GrJA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "_zTHtZGcrK8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history1=model1.fit(train_images, \n",
        "                  train_labels,\n",
        "                  epochs=10,\n",
        "                  validation_data=(val_images,val_labels),\n",
        "                  verbose=2\n",
        "                  ).history"
      ],
      "metadata": {
        "id": "-n4BWtW7rd5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model1, 'model1', history1, X=val_images, y=val_labels, threshold=0.5)"
      ],
      "metadata": {
        "id": "3veuX5CCrg7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Model"
      ],
      "metadata": {
        "id": "iMTYKuKmpi0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = models.Sequential([\n",
        "    # since Conv2D is the first layer of the neural network, we should also specify the size of the input\n",
        "    layers.Conv2D(128, (3,3), activation='relu', input_shape=(input_layer)),\n",
        "    # apply pooling\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    # flatten the result to feed it to the dense layer\n",
        "    layers.Flatten(), \n",
        "    # and define 512 neurons for processing the output coming by the previous layers\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    # dropout layer\n",
        "    layers.Dropout(0.5),\n",
        "    # a single output neuron\n",
        "    layers.Dense(1, activation='sigmoid')  \n",
        "])"
      ],
      "metadata": {
        "id": "KxtcY7bCNumL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "lxdyhZwgRTMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "t0TJFPCIjXBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "eY13OO0QPV0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2=model2.fit(train_images, \n",
        "                  train_labels,\n",
        "                  epochs=10,\n",
        "                  validation_data=(val_images,val_labels),\n",
        "                  verbose=2\n",
        "                  ).history"
      ],
      "metadata": {
        "id": "feTjbiZYRs3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model2, 'model2', history2, X=val_images, y=val_labels, threshold=0.5)"
      ],
      "metadata": {
        "id": "8aCA8xzxh4V-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}